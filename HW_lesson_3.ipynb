{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0d1cd076",
   "metadata": {},
   "source": [
    "## Урок 3. Построение модели классификации."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f6221182",
   "metadata": {},
   "source": [
    "### Домашнее задание"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a17bea6",
   "metadata": {},
   "source": [
    "1. Для чего и в каких случаях полезны различные варианты усреднения для метрик качества классификации: micro, macro, weighted?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6dc9fba",
   "metadata": {},
   "source": [
    "#### Решение:\n",
    "\n",
    "a) **micro** показатель F1-Score — гармоническое среднее.\n",
    "Применяется, когда количества объектов разных классов кратно отличаются.\n",
    "Подсчитывает общее количество истинных срабатываний, false negatives и false positives.\n",
    "\n",
    "b) **macro** показатель — просто средний F1-Score.\n",
    "Плохо работает с несбалансированными классами (когда количество объектов одного класса кратно больше, чем другого).\n",
    "\n",
    "c) **weighted** - тот же macro показатель, но с учетом дисбаланса классов (за счет весовых коэффициентов для каждого класса).\n",
    "\n",
    "$ $"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "084c83ef",
   "metadata": {},
   "source": [
    "2. В чём разница между моделями xgboost, lightgbm и catboost или какие их основные особенности?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "faee5982",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "#### Решение:\n",
    "\n",
    "1. **xgboost (extreme gradient boosting)**  \n",
    "это модернизированный градиентный бустинг, с идеальной комбинацией оптимизации ПО и железа для получения точных релузьтатов за короткое время с минимальным использованием вычислительных ресурсов.\n",
    "Под капотом ансамбли методов деревьев, которые используют принцип бустинга слабых учеников.\n",
    "\n",
    "2. **lightgbm**  \n",
    "относится к классу ансамблевых алгоритмов машинного обучения,\n",
    "которые могут использоваться для задач классификации или регрессионного прогностического\n",
    "моделирования.  \n",
    "Реализация вводит две ключевые идеи: GOSS и EFB.  \n",
    "    - Градиентная односторонняя выборка (GOSS) является модификацией градиентного бустинга,\n",
    "который фокусирует внимание на тех учебных примерах, которые приводят к большему градиенту,\n",
    "в свою очередь, ускоряя обучение и уменьшая вычислительную сложность метода.  \n",
    "С помощью GOSS исключается значительная доля экземпляров данных с небольшими градиентами\n",
    "и используется только остальные экземпляры для оценки прироста информации.\n",
    "Мы доказываем, что, поскольку экземпляры данных с большими градиентами играют более важную\n",
    " роль в вычислении информационного выигрыша, GOSS может получить довольно точную оценку информационного выигрыша с гораздо меньшим размером данных.\n",
    "    - Exclusive Feature Bundling (объединение взаимоисключающих признаков), или EFB, — это подход объединения разрежённых (в основном нулевых) взаимоисключающих признаков, таких как категориальные переменные входных данных, закодированные унитарным кодированием. Таким образом, это тип автоматического подбора признаков.  \n",
    "*Преимущества перед XGBoost*\n",
    "- использует алгоритм на основе гистограммы, то есть он объединяет непрерывные значения признаков в дискретные ячейки, которые ускоряют процедуру обучения.\n",
    "- заменяет непрерывные значения на дискретные ячейки, что приводит к меньшему использованию памяти.\n",
    "- он создает гораздо более сложные деревья, следуя подходу разделения листьев, а не поуровневому подходу, который является основным фактором в достижении более высокой точности. Однако иногда это может привести к переобучению, чего можно избежать, установив параметр max_depth.\n",
    "- одинаково хорошо работает с большими наборами данных при значительном сокращении времени обучения по сравнению с XGBOOST.\n",
    "\n",
    "3. **catboost**  \n",
    "библиотека градиентного бустинга, созданная Яндексом. Использует небрежные (oblivious) деревья решений, чтобы вырастить сбалансированное дерево. Одни и те же функции используются для создания левых и правых разделений (split) на каждом уровне дерева. По сравнению с классическими деревьями, небрежные деревья более эффективны при реализации на процессоре и просты в обучении.  \n",
    "Наиболее распространенными способами обработки категориальных данных в машинном обучении является one-hot кодирование и кодирование лейблов. CatBoost позволяет использовать категориальные признаки без необходимости их предварительно обрабатывать.\n",
    "При использовании CatBoost мы не должны пользоваться one-hot кодированием, поскольку это влияет на скорость обучения и на качество прогнозов. Вместо этого мы просто задаем категориальные признаки с помощью параметра cat_features.\n",
    "*Преимущества использования CatBoost*\n",
    "- CatBoost позволяет проводить обучение на нескольких GPU.\n",
    "- Библиотека позволяет получить отличные результаты с параметрами по умолчанию, что сокращает время, необходимое для настройки гиперпараметров.\n",
    "- Обеспечивает повышенную точность за счет уменьшения переобучения.\n",
    "- Возможность быстрого предсказания с применением модели CatBoost;\n",
    "- Обученные модели CatBoost можно экспортировать в Core ML для вывода на устройстве (iOS).\n",
    "- Умеет под капотом обрабатывать пропущенные значения.\n",
    "- Может использоваться для регрессионных и классификационных задач.\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
